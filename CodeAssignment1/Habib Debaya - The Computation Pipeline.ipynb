{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZk5t2U6uLCZ"
      },
      "source": [
        "\n",
        "# The Frozen Lake Environment (Mendes 2019)\n",
        "In this article, we are going to learn how to create and explore the \n",
        "Frozen Lake environment using the Gym library, an open source project created by OpenAI used \n",
        "for reinforcement learning experiments.The Gym library defines a uniform interface for environments \n",
        "what makes the integration between algorithms and environment easier for developers. \n",
        "Among many ready-to-use environments, the default installation includes a text-mode version of \n",
        "the Frozen Lake game, used as example in our last post.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysqSN8kVuGnt",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "#Importing the gym library and creating the environment\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "env.reset()                    \n",
        "env.render() \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhp4QN5Rucpu"
      },
      "source": [
        "The output is a matrix of tiles showing the player's current state, some tiles are holes, \n",
        "others are frozen. Finally, there's a Goal tile that the agent should try to reach. \n",
        "Also, we can inspect the possible actions to perform in the environment, \n",
        "as well as the possible states of the game.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1as7QEsuGnu",
        "outputId": "e0ae4452-5030-4251-d9fe-339816ba4a5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action space:  Discrete(4)\n",
            "Observation space:  Discrete(16)\n"
          ]
        }
      ],
      "source": [
        "print(\"Action space: \", env.action_space)\n",
        "print(\"Observation space: \", env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmKOcB5Iutmk"
      },
      "source": [
        "In the code above, we print on the console the field action_space and the field observation_space. \n",
        "The returned objects are of the type Discrete, which describes a discrete space of size n. \n",
        "For example, the action_space for the Frozen Lake environment is a discrete space of 4 values, \n",
        "which means that the possible values for this space are 0 (zero), 1, 2 and 3. \n",
        "Yet, the observation_space is a discrete space of 16 values, which goes from 0 to 15. \n",
        "Besides, these objects offer some utility methods, like the sample() method which returns a random value \n",
        "from the space. With this method, \n",
        "we can easily create a dummy agent that plays the game randomly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q05JJWb_uGnw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "MAX_ITERATIONS = 10\n",
        " \n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "env.reset()\n",
        "env.render()\n",
        "for i in range(MAX_ITERATIONS):\n",
        "    random_action = env.action_space.sample()\n",
        "    new_state, reward, done, info = env.step(\n",
        "       random_action)\n",
        "    env.render()\n",
        "    if done:\n",
        "        break\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGUt6lNKvGcw"
      },
      "source": [
        "The code above executes the game for a maximum of 10 iterations using the method sample() from the action_space object to select a random action. Then the env.step() method takes the action as input, executes the action on the environment and returns a tuple of four values:\n",
        "\n",
        "*   **new_state:** The new state of the environment\n",
        "*   **reward:** The reward\n",
        "*   **done:** A boolean flag indicating if the returned state is a terminal state\n",
        "*   **info:** An object with additional information for debugging purposes\n",
        "\n",
        "Finally, we use the method env.render() to print the grid on the console and use the returned “done” flag to \n",
        "break the loop. Notice that the selected action is printed together with the grid.\n",
        "\n",
        "# **Stochastic vs Deterministic**\n",
        "Note in the previous output the cases in which the player moves in a different direction than the one chosen by the agent. This behavior is completely normal in the Frozen Lake environment because it simulates a slippery surface. Also, this behavior represents an important characteristic of real-world environments: the transitions from one state to another, for a given action, are probabilistic. For example, if we shoot a bow and arrow there’s a chance to hit the target as well as to miss it. The distribution between these two possibilities will depend on our skill and other factors, like the direction of the wind, for example. Due to this probabilistic nature, the final result of a state transition does not depend entirely on the taken action.\n",
        "\n",
        "By default, the Frozen Lake environment provided in Gym has probabilistic transitions between states. In other words, even when our agent chooses to move in one direction, the environment can execute a movement in another direction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW7S9mJDuGnx",
        "outputId": "196aafa0-2e6b-4f58-bf2b-6c356781354e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- winning sequence ------ \n",
            "['Right', 'Right', 'Down', 'Down', 'Down', 'Right']\n",
            "\n",
            "Reward: 0.00\n",
            "{'prob': 0.3333333333333333}\n",
            "\n",
            "Reward: 0.00\n",
            "{'prob': 0.3333333333333333}\n",
            "\n",
            "Reward: 0.00\n",
            "{'prob': 0.3333333333333333}\n",
            "\n",
            "Reward: 0.00\n",
            "{'prob': 0.3333333333333333}\n",
            "\n",
            "Reward: 0.00\n",
            "{'prob': 0.3333333333333333}\n",
            "\n",
            "Reward: 0.00\n",
            "{'prob': 0.3333333333333333}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "actions = {\n",
        "    'Left': 0,\n",
        "    'Down': 1,\n",
        "    'Right': 2, \n",
        "    'Up': 3\n",
        "}\n",
        " \n",
        "print('---- winning sequence ------ ')\n",
        "winning_sequence = (2 * ['Right']) + (3 * ['Down'])+ ['Right']\n",
        "print(winning_sequence)\n",
        " \n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "env.reset()\n",
        "env.render()\n",
        " \n",
        "for a in winning_sequence:\n",
        "    new_state, reward, done, info = env.step(actions[a])\n",
        "    print()\n",
        "    env.render()\n",
        "    print(\"Reward: {:.2f}\".format(reward))\n",
        "    print(info)\n",
        "    if done:\n",
        "        break  \n",
        " \n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMvNdnuiwU-w"
      },
      "source": [
        "Executing the code above, we can observe different results and paths at each execution. Also, using the info object returned by the step method we can inspect the probability used by the environment to choose the executed movement.\n",
        "\n",
        "\n",
        "However, the Frozen Lake environment can also be used in deterministic mode. By setting \n",
        "the property is_slippery=False when creating the environment, the slippery surface is turned \n",
        "off and then the environment always executes the action chosen by the agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkJAmQRLuGny",
        "outputId": "1fc884ac-81ef-4b6c-c3d0-20c3cb3931b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Reward: 0.00\n",
            "{'prob': 1.0}\n",
            "\n",
            "Reward: 0.00\n",
            "{'prob': 1.0}\n",
            "\n",
            "Reward: 0.00\n",
            "{'prob': 1.0}\n",
            "\n",
            "Reward: 0.00\n",
            "{'prob': 1.0}\n",
            "\n",
            "Reward: 0.00\n",
            "{'prob': 1.0}\n",
            "\n",
            "Reward: 1.00\n",
            "{'prob': 1.0}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "env.reset()\n",
        "env.render()\n",
        " \n",
        "for a in winning_sequence:\n",
        "    new_state, reward, done, info = env.step(actions[a])\n",
        "    print()\n",
        "    env.render()\n",
        "    print(\"Reward: {:.2f}\".format(reward))\n",
        "    print(info)\n",
        "    if done:\n",
        "        break\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjNKQgZsxGrC"
      },
      "source": [
        "# **Map sizes and custom maps** \n",
        "The default 4×4 map is not the only option to play the Frozen Lake game. \n",
        "Also, there’s an 8×8 version that we can create in two different ways. \n",
        "The first one is to use the specific environment id for the 8×8 map:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HP4-A6SuGnz"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"FrozenLake8x8-v1\")\n",
        "env.reset()\n",
        "env.render()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q2eSxJUxLne"
      },
      "source": [
        "The second option is to call the make method \n",
        "passing the value “8×8” as an argument to the map_name parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZiZjHQXuGnz"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', map_name='8x8')\n",
        "env.reset()\n",
        "env.render()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCnBQdAkxUVa"
      },
      "source": [
        "And finally, we can create our custom map of the Frozen Lake game by passing \n",
        "an array of strings representing the map as an argument to the parameter desc:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjycffkquGn0"
      },
      "outputs": [],
      "source": [
        "\n",
        "custom_map = [\n",
        "    'SFFHF',\n",
        "    'HFHFF',\n",
        "    'HFFFH',\n",
        "    'HHHFH',\n",
        "    'HFFFG'\n",
        "]\n",
        " \n",
        "env = gym.make('FrozenLake-v1', desc=custom_map)\n",
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyVMCxJ9wntS"
      },
      "source": [
        "\n",
        "# **Conclusion**\n",
        "In this post, we learned how to use the Gym library to create an environment to train a \n",
        "reinforcement learning agent. We focused on the Frozen Lake environment, a text mode game with \n",
        "simple rules but that allows us to explore the fundamental concepts of reinforcement learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxuWXq2P1QAw"
      },
      "source": [
        "# Research Questions\n",
        "\n",
        "## Question 1:\n",
        "\n",
        "The gym library is a collection of simulated environments that can be used to develop and test reinforcement learning algorithms through a simple interface. The framework provides an \"Environment\" object, the intelligent agent''s actuators interact with the Environment object through the \"step\" function. This is a description of the return values (OpenAi 2016):\n",
        "\n",
        "* observation (object): an environment-specific object representing your observation of the environment. For example, pixel data from a \n",
        "camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
        "* reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
        "* done (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
        "* info (dict): diagnostic information useful for debugging.  It can sometimes be useful for learning  (for example, it might contain the raw probabilities behind the environment’s last state change). \n",
        "\n",
        "This is a useful abstraction on the classic \"Agent-Environment\" interaction; The agent interacts with the environment, the environment returns \"metadata\" about the action (Observation + Reward) that the agent uses to inform its next decision based on the algorithm employed.\n",
        "\n",
        "The Environment interface also provides both the set of possible actions in a given position as well as the set of all possible states in the game through the \"action_space\" and \"observation_space\" attributes respectively.\n",
        "\n",
        "These can be useful for checking if a given action is valid or sampling from the action_space to simulate random trials.\n",
        "\n",
        "Fundamentally, the difference between the enviornment and the agent is that the former is the objective situation and the latter is the subjective surrogate. The environment is set up so that the agent can interact with it. The environment is largely static: it has a number of paramteres such as \"state,\" \"action,\" and \"reward.\" The agent then has a responsibility to interact with the environment is its own specific way. It will try to analyze the landscape to find the *optimal policy* for its own personal objectives.\n",
        "\n",
        "## Question 2:\n",
        "\n",
        "Markdown is a powerful and simple tool to style your text according to your needs, offering great support across a range of devices. The main principle for using Markdown when sharing your code is to make sure that your code is easily readable by other humans. Indenting your line in by the use of tab, leaving empty lines in between paragraphs, and bolding, italicizing, and underlying important information all serve to more professionally convey your information to the reader. First, in order to have principled markdown, one must learn how to use the tool. Two references for markdown tutorials are markdownguide.org and markdowntutorial.com. As for the principles of writing lucid communication as part of your code, a useful guide is the Engineer’s Guide to Writing Code Comments available [here](https://www.stepsize.com/blog/the-engineers-guide-to-writing-code-comments). In it, the author emphasizes the importance of writing comments as you go, so as to not get bogged down in increased complexity and to keep your references contained in the current document.\n",
        "\n",
        "\n",
        "## Question 3:\n",
        "\n",
        "For my assignment, I used Colab. Colab was a great tool as it allowed me to have cells for code and cells for text, allowing for different programming languages for each use.\n",
        "The principle for effective storytelling using the Frozen-AI environment in openAI gym is to explain the intuition of the algorithm in every step of the way. The reader should be introduced, through a text cell, to the what, the how and the why of each code cell. In this way, the logical reasoning of the author is transmitted to the reader through logical steps, without any jumps or holes in mutual understanding. This is very useful when the story is complex as the reader can be lost, lose interest, and therefore lucid communication is not achieved. A good example in The Frozen Lake article (Mendes 2019) is the Stochastic vs Deterministic section. The act of contrasting the two different settings gracefully elucidates the probabilistic aspect of the random trials.\n",
        "\n",
        "# References:\n",
        "\n",
        "Mendes, R. (2019, June 16). Gym Tutorial: The Frozen Lake. Reinforcement Learning for Fun. Retrieved April 3, 2022, from https://reinforcement-learning4.fun/2019/06/16/gym-tutorial-frozen-lake/\n",
        "\n",
        "Gym: A toolkit for developing and comparing reinforcement learning algorithms. (2016). OpenAI. Retrieved April 3, 2022, from https://gym.openai.com/docs/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Habib Debaya  - The Computational Pipline",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}