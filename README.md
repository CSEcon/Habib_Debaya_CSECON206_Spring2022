RQ1: What is cooperative AI? What do you see as the potential that computer science and economics can jointly contribute to advance cooperative AI?

According to Cooperative AI's website, Cooperative AI aims to reconceive AI as deeply social. More practically, cooperative AI wants to find ways where AI is not engaged in a zero-sum conflict game but rather a game where common and conflicting interests are present and AI has to find ways to work with the opposite party, whether it is another AI or a human. I think the contribution of Computer Science will center on the AI algorithms. Better deep learning models will allow AI to process more information at higher speeds so that it can make more accurate information faster. As for Economics, I believe that the game theory models will be very useful in guiding AI towards the best cooperative decisions to make during games.


RQ2: Besides the desirable outcomes of cooperation, what other desirable outcomes the mechanism design theory aims at achieving?


Mechanism design, at its heart, wants to allow agents who are self-interested and with incomplete information to make decisions that create utilitarian good. I believe that Mechanism Design (MD) is a very virtuous field of study as it finds ways to channel selfish human behavior to achieve positive outcomes. MD's particular power is that it takes into account that agents are self-interested and tries to work around that to create as much positivity as possible. By regulating specific mechanism related to specific outcomes, and with putting forth the right incentives (usually money), MD channels self-interest and lack of information to create social good.


RQ3: What are the limitations of the current mechanism design theory in achieving desirable outcomes? What new challenging are we facing in a new era with more and more human and AI interactions?


Impossibility theorems, namely Arrow's impossibility theorem and the Gibbard–Satterthwaite theorem, represent limitations to current mechanism design theory. Both theorems revolve around the fact that voting systems may not produce the desired result of voters even if every voter votes sincerely. These impossibility theorems present an issue to Mechanism Design. Since MD attempts to devise systems that produce positive results in spite of self-interest and incomplete information, if an impossibility theory states that some models can never produce the wanted results, it means that Mechanism Design cannot apply to all models, as some of them are inherently misleading. As more human and AI interactions happen, we might find ourselves in games that are certain to produce results that are not ideal. As such, we should not only create cooperative AI but also make sure that the games we play have a possibility of achievieng desirable outcomes.


References

Dafoe, Allan, Yoram Bachrach, Gillian Hadfield, Eric Horvitz, Kate Larson, and Thore Graepel. 2021. “Cooperative AI: Machines Must Learn to Find Common Ground.” Nature News. Nature Publishing Group. May 4. https://www.nature.com/articles/d41586-021-01170-0. 

Chen, James. 2022. “Mechanism Design Theory.” Investopedia. Investopedia. February 8. https://www.investopedia.com/terms/m/mechanism-design-theory.asp. 

“Arrow's Impossibility Theorem.” 2021. Wikipedia. Wikimedia Foundation. December 13. https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem. 

“Gibbard–Satterthwaite Theorem.” 2022. Wikipedia. Wikimedia Foundation. April 15. https://en.wikipedia.org/wiki/Gibbard%E2%80%93Satterthwaite_theorem. 


